import copy

import numpy as np
import torch
from torch import optim

from aux_task_discovery.utils import random_argmax
import aux_task_discovery.utils.pytorch_utils as ptu
from aux_task_discovery.agents.base import BaseAgent, ReplayBuffer
from aux_task_discovery.agents.gen_test.generators import Generator
from aux_task_discovery.agents.gen_test.testers import Tester
from aux_task_discovery.models import MasterUserNetwork


class GenTestAgent(BaseAgent):
    '''
    DQN agent with mutliple output heads for action values corresponding to each of 
    a given number of auxillery tasks. Auxillery tasks are generated by the given
    generator class and their utility is evaluated by the given tester class.
    Algorithm specified in Algorithm 1 @ https://arxiv.org/abs/2210.14361
    '''
    def __init__(
        self,
        input_shape: tuple,
        n_actions: int,
        generator: str,
        tester: str,
        n_aux_tasks = 5,
        age_threshold = 0,
        replace_cycle = 500,
        replace_ratio = 0.2,
        trace = 0.05,
        seed = 42,
        learning_rate = 0.0001, 
        epsilon = 0.1,
        anneal_epsilon = False,
        n_anneal = 10000,
        gamma = 0.9,
        hidden_size = 500,
        activation = 'tanh',
        buffer_size = 1000,
        batch_size = 16,
        update_freq = 1,
        target_update_freq=100,
        learning_start = 100,
    ):
        self.n_actions = n_actions
        self.n_aux_tasks = n_aux_tasks
        self.age_threshold = age_threshold,
        self.replace_cycle = replace_cycle,
        self.replace_ratio = replace_ratio,
        self.task_ages = np.zeros(n_aux_tasks)
        self.rand_gen = np.random.RandomState(seed)
        self.epsilon = epsilon
        self.anneal_epsilon = anneal_epsilon
        self.n_anneal = n_anneal
        self.gamma = gamma
        self.batch_size = batch_size
        self.update_freq = update_freq
        self.target_update_freq = target_update_freq
        self.learning_start = learning_start
        self.step_idx = 1
        self.generator = None #TODO Get generator from str and init
        self.tester = None # TODO Get tester from str and init
        self.replay_buffer = ReplayBuffer(capacity=buffer_size, seed=seed)
        self.model = MasterUserNetwork(
                        input_shape=input_shape,
                        n_actions=n_actions,
                        n_heads=n_aux_tasks+1, # Add additional head for main task
                        hidden_size=hidden_size,
                        activation=activation,
                        )
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.tasks = np.array(generator.generate_tasks(self.n_aux_tasks))
        self._update_target_network()

    def _update_target_network(self):
        self.target_model = copy.deepcopy(self.model)

    @torch.no_grad()
    def get_action(self, obs: np.ndarray):
        '''
        Selects e-greedy action using q-values for main task from model
        '''
        if self.rand_gen.rand() < self.epsilon:
            return self.rand_gen.randint(0, self.n_actions)
        obs = ptu.from_numpy(obs)
        obs = obs.unsqueeze(0)
        q_vals = ptu.to_numpy(self.model(obs))['main'][0]
        return random_argmax(q_vals)

    def get_loss(self):
        '''
        Samples batch from replay buffer sums MSE loss for each output head.
        '''
        #TODO Check if this should be MSE over all output heads
        batch = self.replay_buffer.sample(batch_size=self.batch_size)
        obs = batch["observations"]
        act = batch["actions"]
        rew = batch["rewards"]
        next_obs = batch["next_observations"]
        terminated = batch["terminateds"]
        truncated = batch["truncateds"]
        
        # TODO Set self.tasks using self.generator
        for task_id, task in self.tasks:
            # Get max state-action values for next states from target net
            next_q = self.target_model(ptu.from_numpy(next_obs))[task_id].max(dim=-1)[0].detach() #TODO Assumes forward returns dict for each head output with task_id
            next_q[terminated & ~truncated] = 0
            targets = ptu.from_numpy(rew) + (self.gamma * next_q)

        # Get pred q_vals for current obs
        preds = self.model(ptu.from_numpy(obs))[torch.arange(obs.shape[0]), ptu.from_numpy(act)]
        
        # Calculate MSE
        losses = (targets - preds) ** 2
        loss = losses.mean()
        return loss